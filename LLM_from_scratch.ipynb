{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNlAxo4z548nsrF+2Gmi20L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ijasah/notebooks/blob/main/LLM_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcRU48cENQhz"
      },
      "outputs": [],
      "source": [
        "# Installation block\n",
        "!pip install transformers datasets sentencepiece\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load a Small Dataset\n",
        "We'll use wikitext (which contains Wikipedia articles) and take a small portion."
      ],
      "metadata": {
        "id": "0HVyWxEbNwTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "# Load GPT-2 tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "# Set padding token to eos_token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "# Tokenize function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=64)\n",
        "# Apply tokenization\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "# Convert to PyTorch format\n",
        "import torch\n",
        "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "# Check tokenized sample\n",
        "print(tokenized_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2zz9nTSPgyk",
        "outputId": "65a33b39-cc34-470b-d769-e81eb58f123c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256]), 'attention_mask': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Extract input_ids and attention_mask\n",
        "input_ids = tokenized_dataset[\"input_ids\"]\n",
        "attention_mask = tokenized_dataset[\"attention_mask\"]\n",
        "# Convert to PyTorch tensors correctly\n",
        "input_ids = torch.tensor(tokenized_dataset[\"input_ids\"])\n",
        "attention_mask = torch.tensor(tokenized_dataset[\"attention_mask\"])\n",
        "# Create dataset and DataLoader\n",
        "dataset = TensorDataset(input_ids, attention_mask)\n",
        "train_loader = DataLoader(dataset, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ln658ObsQ3yJ",
        "outputId": "092ad629-e786-4cea-e1b1-a1f8a181c02a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-d2af68e791f4>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_ids = torch.tensor(tokenized_dataset[\"input_ids\"])\n",
            "<ipython-input-17-d2af68e791f4>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  attention_mask = torch.tensor(tokenized_dataset[\"attention_mask\"])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm  # For progress bar\n",
        "\n",
        "# Define Model Again (if not defined earlier)\n",
        "# class TinyTransformer(nn.Module):\n",
        "#     def __init__(self, vocab_size, d_model=128, n_heads=4, num_layers=2, max_length=64):\n",
        "#         super().__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "#         self.pos_embedding = nn.Embedding(max_length, d_model)\n",
        "\n",
        "#         encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads)\n",
        "#         self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "#         self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         seq_length = x.shape[1]\n",
        "#         pos = torch.arange(0, seq_length).unsqueeze(0).to(x.device)\n",
        "\n",
        "#         x = self.embedding(x) + self.pos_embedding(pos)\n",
        "#         x = self.transformer(x)\n",
        "#         logits = self.fc_out(x)\n",
        "\n",
        "#         return logits\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "class TinyTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, n_heads=8, num_layers=4, max_length=64, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_embedding = nn.Embedding(max_length, d_model)\n",
        "\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, activation=\"gelu\", norm_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        self.fc_out.weight = self.embedding.weight  # Weight tying\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_length = x.shape[1]\n",
        "        pos = torch.arange(0, seq_length).unsqueeze(0).to(x.device)\n",
        "\n",
        "        x = self.embedding(x) + self.pos_embedding(pos)\n",
        "        x = self.norm(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.transformer(x)\n",
        "        logits = self.fc_out(x)\n",
        "\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "7-oljLPcRVPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "vocab_size = len(tokenizer)\n",
        "model = TinyTransformer(vocab_size).to(device)\n",
        "\n",
        "# Define Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
        "\n",
        "# Training Loop with Progress Bar\n",
        "num_epochs = 10  # Small dataset, so only 3 epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        input_ids, attention_mask = batch\n",
        "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids)\n",
        "\n",
        "        # Shift labels so that each token predicts the next token\n",
        "        labels = input_ids[:, 1:].contiguous()\n",
        "        logits = logits[:, :-1, :].contiguous()\n",
        "\n",
        "        loss = criterion(logits.view(-1, vocab_size), labels.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    print(f\"âœ… Epoch {epoch+1} Completed | Avg Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "print(\"ðŸŽ¯ Model Training Completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5K65R7s6Z5i-",
        "outputId": "ff024bf0-bbb9-40e0-fd7a-77020b21fd9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4590/4590 [02:42<00:00, 28.33it/s, loss=2.54]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Epoch 1 Completed | Avg Loss: 4.5991\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4590/4590 [02:40<00:00, 28.61it/s, loss=4.69]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Epoch 2 Completed | Avg Loss: 3.2640\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4590/4590 [02:39<00:00, 28.72it/s, loss=5.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Epoch 3 Completed | Avg Loss: 3.0971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4590/4590 [02:39<00:00, 28.72it/s, loss=4.61]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Epoch 4 Completed | Avg Loss: 2.9878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4590/4590 [02:39<00:00, 28.70it/s, loss=1.12]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Epoch 5 Completed | Avg Loss: 2.9025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4590/4590 [02:40<00:00, 28.63it/s, loss=2.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Epoch 6 Completed | Avg Loss: 2.8358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4590/4590 [02:40<00:00, 28.56it/s, loss=1.21]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Epoch 7 Completed | Avg Loss: 2.7813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4590/4590 [02:39<00:00, 28.73it/s, loss=1.83]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Epoch 8 Completed | Avg Loss: 2.7368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4590/4590 [02:40<00:00, 28.55it/s, loss=1.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Epoch 9 Completed | Avg Loss: 2.7006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4590/4590 [02:40<00:00, 28.56it/s, loss=3.26]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Epoch 10 Completed | Avg Loss: 2.6683\n",
            "ðŸŽ¯ Model Training Completed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "# torch.save(model.state_dict(), \"tiny_transformer.pth\")\n",
        "torch.save(model, \"tiny_transformer.pth\")\n",
        "# Save tokenizer\n",
        "tokenizer.save_pretrained(\"tiny_tokenizer\")\n",
        "print(\"âœ… Model and Tokenizer Saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzi_OmMVSA4z",
        "outputId": "30513c5f-6c81-4005-ecaf-22d25e067d90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model and Tokenizer Saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "import torch.nn as nn\n",
        "\n",
        "# âœ… Load tokenizer\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"tiny_tokenizer\")\n",
        "# âœ… Load model with `weights_only=False` and allowlist `TinyTransformer`\n",
        "torch.serialization.add_safe_globals([TinyTransformer])  # Allow the custom class\n",
        "model = torch.load(\"tiny_transformer.pth\", weights_only=False)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "print(\"âœ… Model and Tokenizer Loaded Successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOvrh7XYSl-Q",
        "outputId": "563280c6-19d4-421d-e32e-6aaa0f9804b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model and Tokenizer Loaded Successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "# def generate_text(prompt, max_length=20):\n",
        "#     model.eval()\n",
        "\n",
        "#     input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for _ in range(max_length):\n",
        "#             logits = model(input_ids)[:, -1, :]\n",
        "#             next_token = torch.argmax(logits, dim=-1).unsqueeze(0)\n",
        "#             input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "#     return tokenizer.decode(input_ids.squeeze().tolist())\n",
        "\n",
        "# # Generate a sentence\n",
        "# print(generate_text(\"Who is the president of\"))\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "def sample_next_token(logits, temperature=1.0, top_k=50, top_p=0.9):\n",
        "    \"\"\" Applies temperature scaling, top-k and top-p filtering to select the next token. \"\"\"\n",
        "\n",
        "    # Apply temperature scaling\n",
        "    logits = logits / temperature\n",
        "\n",
        "    # Convert logits to probabilities\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "    # Top-k filtering (only keep top k highest probability tokens)\n",
        "    if top_k > 0:\n",
        "        values, indices = torch.topk(probs, top_k)\n",
        "        probs = torch.zeros_like(probs).scatter_(-1, indices, values)\n",
        "\n",
        "    # Top-p (nucleus) sampling\n",
        "    if top_p < 1.0:\n",
        "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above top_p\n",
        "        sorted_probs[cumulative_probs > top_p] = 0\n",
        "        probs = torch.zeros_like(probs).scatter_(-1, sorted_indices, sorted_probs)\n",
        "\n",
        "    # Normalize probabilities after filtering\n",
        "    probs = probs / probs.sum()\n",
        "\n",
        "    # Sample the next token\n",
        "    next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "    return next_token\n",
        "\n",
        "def generate_text(prompt, max_length=20, temperature=1.0, top_k=50, top_p=0.9):\n",
        "    \"\"\" Generates text using a Transformer model with temperature, top-k, and top-p sampling. \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            logits = model(input_ids)[:, -1, :]\n",
        "            next_token = sample_next_token(logits, temperature, top_k, top_p)\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "    return tokenizer.decode(input_ids.squeeze().tolist())\n",
        "\n",
        "# Example usage\n",
        "print(generate_text(\"The game began development in 2010\", temperature=0.8, top_k=40, top_p=0.7))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcjLcg-RStC0",
        "outputId": "37fc4eec-92a4-4d47-94d8-1959d0575f69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The game began development in 2010 and a school is been a early a early , I who of the episode of the early War \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-E8bazszU4Dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6BKDg1bPra0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nCehlf-Zra70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vzpjTFgfra_-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}